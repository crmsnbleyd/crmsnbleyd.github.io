
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="./theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
    href="./theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
          href="./theme/pygments/monokai.min.css">



  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="./theme/font-awesome/css/solid.css">












 

<meta name="author" content="Arnav Andrew Jose" />
<meta name="description" content="What pre-training is and how it is done." />
<meta name="keywords" content="">


  <meta property="og:site_name" content="Language Modelling Course"/>
  <meta property="og:title" content="Pre-training"/>
  <meta property="og:description" content="What pre-training is and how it is done."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="./pre-training.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2022-12-04 09:33:00+05:30"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="./author/arnav-andrew-jose.html">
  <meta property="article:section" content="Terminology"/>
  <meta property="og:image" content="">

  <title>Language Modelling Course &ndash; Pre-training</title>


</head>
<body class="dark-theme">

<aside>
  <div>
    <a href="./">
      <img src="./theme/img/profile.png" alt="" title="">
    </a>

    <h1>
      <a href="./"></a>
    </h1>



    <nav>
      <ul class="list">


            <li>
              <a target="_self"
                 href="./pages/about.html#about">
                About
              </a>
            </li>

          <li>
            <a target="_self" href="/category/introduction.html" >Introduction</a>
          </li>
          <li>
            <a target="_self" href="/category/terminology.html" >Terminology</a>
          </li>
          <li>
            <a target="_self" href="/category/neural_networks.html" >Neural Networks</a>
          </li>
          <li>
            <a target="_self" href="/category/models.html" >Models</a>
          </li>
          <li>
            <a target="_self" href="https://huggingface.co" >Huggingface</a>
          </li>
          <li>
            <a target="_self" href="https://pytorch.org/docs/stable/index.html" >Pytorch Documentation</a>
          </li>
      </ul>
    </nav>

    <ul class="social">
      <li>
        <a class="sc-Github"
           href="https://github.com/crmsnbleyd"
           target="_blank">
          <i class="fa-brands fa-Github"></i>
        </a>
      </li>
      <li>
        <a class="sc-Linkedin"
           href="https://www.linkedin.com/in/arnav-andrew-jose"
           target="_blank">
          <i class="fa-brands fa-Linkedin"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="pre-training">Pre-training</h1>
    <p>
      Posted on Sun 04 December 2022 in <a href="./category/terminology.html">Terminology</a>

    </p>
  </header>


  <div>
    <p>Pre-training in AI refers to training a model with one task to help it form parameters that can be used in other tasks.</p>
<p>The language models we will be dealing with are trained in a specific way to solve an artificial task (for example, guessing blanks in sentences for <a href="./bert.html">BERT</a>), and that knowledge is used in order to solve other tasks.</p>
<p>BERT and <a href="./electra.html">ELECTRA</a> are two pre-training techniques used for the same structure of nodes, and both of them use <em>unstructured data</em>, that is, data we do not have to label ourselves, in the form of a large corpus of text that we can obtain from the internet.</p>
<p>Pre-training involves setting the number of parameters and layers required, pointing the training code to the text used to train, and letting it run and refine the network for a while. This can take days.</p>
<p>The training code for many popular models like ELECTRA are available online on <a href="https://huggingface.co">Hugging Face</a>, and only need to be modified slighty, easing our task considerably.</p>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>






</article>

<footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Language Modelling Course ",
  "url" : ".",
  "image": "",
  "description": ""
}
</script>
</body>
</html>